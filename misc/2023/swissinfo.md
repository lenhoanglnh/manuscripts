# A democratic oversight on AI is urgently needed

#### We desperately need more attention, manpower and funding to set up governance systems akin to those introduced in the airline, pharmaceutical and food industries.

## AIs are out of (democratic) control
On March 29, an [open letter](https://futureoflife.org/open-letter/pause-giant-ai-experiments/) calling to "pause giant AI experiments" was signed by 1,415 academics and tech leaders. This letter is long overdue. Two years earlier, my colleagues and I already called for a moratorium in [a paper](https://arxiv.org/abs/2209.15259) based on mathematical security impossibility theorems, whose [publication was long delayed by Google](https://tournesol.app/entities/yt:bNfZ0yhVccw).

Over the last decade, ever more impressive algorithms have been hastily developed and deployed on massive scales, like [ChatGPT](https://www.swissinfo.ch/eng/machines-and-ethics--artificial-intelligence-switzerland/46213634) and [MidJourney](https://www.theverge.com/2023/3/30/23662940/deepfake-viral-ai-misinformation-midjourney-stops-free-trials). 
These algorithms rely on ever larger machine learning systems, trained on ever larger amounts of data.

Such algorithms are used in many industries, from [fraud detection](https://www.swissinfo.ch/eng/business/fintech_ai-in-banking--the-reality-behind-the-hype/44046934) to [resume filtering](https://www.forbes.com/sites/tomaspremuzic/2018/05/27/four-unethical-uses-of-ai-in-recruitment/), [video surveillance](https://www.swissinfo.ch/eng/reuters/china-uses-ai-software-to-improve-its-surveillance-capabilities/47501490) and [customer service](https://www.forbes.com/sites/forbesbusinessdevelopmentcouncil/2023/03/27/how-can-ai-fit-into-customer-service-call-centers-effectively/) (despite known [shortcomings](https://www.wired.com/story/welfare-fraud-industry/) and [biases](https://www.theguardian.com/technology/2023/mar/27/robot-recruiters-can-bias-be-banished-from-ai-recruitment-hiring-artificial-intelligence)). But their main application may be in marketing. After all, many of today's biggest technology companies like Google, TikTok and Meta, mostly profit from ad targeting. In fact, [OpenAI's first public customer was none other than Coca-Cola](https://twitter.com/gdb/status/1628122763847413760).

However, AIs have been shown to [spread misinformation](https://www.nytimes.com/2023/02/08/technology/ai-chatbots-disinformation.html), normalize [pseudo-medicines](https://www.fda.gov/consumers/consumer-updates/recipe-danger-social-media-challenges-involving-medicines), amplify [cyberbullying](https://c-hit.org/2019/08/12/social-medias-role-in-cyberbullying/), [endanger mental health](https://jonathanhaidt.substack.com/p/international-mental-illness-part-one), facilitate [illegal (even slavery) markets](https://edition.cnn.com/2021/10/25/tech/facebook-instagram-app-store-ban-human-trafficking/index.html), [favor hate and anger](https://www.technologyreview.com/2021/10/05/1036519/facebook-whistleblower-frances-haugen-algorithms/), produce [deep fakes](https://slate.com/technology/2021/09/deepfake-video-scams.html), [be manipulated by state actors](https://forbiddenstories.org/story-killers/insider/), destabilize [democracies](https://www.theatlantic.com/ideas/archive/2022/07/social-media-harm-facebook-meta-response/670975/) and even contribute to genocides, as asserted by [the United Nations](https://www.theguardian.com/technology/2018/mar/13/myanmar-un-blames-facebook-for-spreading-hatred-of-rohingya) and [Amnesty International](https://www.amnesty.org/en/latest/news/2022/09/myanmar-facebooks-systems-promoted-violence-against-rohingya-meta-owes-reparations-new-report/).
AIs have become a serious national security risk.

Yet AI development has been exceedingly opaque. Hardly any external entity can peek at Google, Meta or OpenAI's actual algorithms. In fact, internal opposition forces have literally been removed, as [Google fired its AI ethics team](https://www.swissinfo.ch/eng/business/what-happens-when-google-fires-its-ethics-/46472076), [Meta dismantled its responsible innovation team](https://www.wsj.com/articles/facebook-parent-meta-platforms-cuts-responsible-innovation-team-11662658423), and [Microsoft laid off an ethics team](https://techcrunch.com/2023/03/13/microsoft-lays-off-an-ethical-ai-team-as-it-doubles-down-on-openai/), after they alarmed about the hasty development of AIs, as in the infamous ["Stochastic Parrot" paper](https://www.theverge.com/2023/3/30/23662940/deepfake-viral-ai-misinformation-midjourney-stops-free-trials), in the [Facebook Files leak](https://www.wsj.com/articles/the-facebook-files-11631713039) or by [Twitter's former head of security](https://www.npr.org/2022/09/13/1122671582/twitter-whistleblower-mudge-senate-hearing).
Powerful profit-seeking companies have successfully engineered a world state where their AIs can be designed with hardly any accountability.

## An effective AI governance is urgently needed
The AI industry is far from being the first out-of-control technology industry. For decades, the airline, car, pharmaceutical, food, tobacco, construction and energy industries, among many others, have commercialized "innovative" unchecked products, which cost millions of lives. Experts eventually opposed this dangerous lack of accountability. Now, in all democracies, strict laws and powerful well-funded regulatory agencies have been set up to enforce a democratic control over the development of new technologies in these areas. A similar oversight on the software industry is long overdue.

In particular, it is high time that we all favor *secure* and *ethical* technologies, rather than celebrate our countries' supposedly leading position in "spectacular AIs". Concretely, the impressiveness of the AIs running our smartgrids, cars, planes, power stations, banks, data centers and social networks, should matter a lot less than their reliability. If these algorithms are brittle, vulnerable, hacked or outsourced to an unreliable provider ([which they often are](https://www.theregister.com/2023/03/23/critical_infrastructure_hardware_flaws/)), or if they abuse human rights, then all of our society is great danger. As asserted by New York Times journalist Nicole Perlroth in her latest book, [this is how they tell me the world ends](https://thisishowtheytellmetheworldends.com/).

Unfortunately, the current culture in the software industry and among AI scholars, as well as the current legal and economic incentives, do not favor a security mindset. Too often, the more cited, celebrated and funded researchers, the more paid software positions and the more successful companies, are those who do not spend much time pondering security and ethical concerns. This must urgently change. 

Our democracies probably cannot afford the decades that were required to set up laws and inspection agencies in other industries. Given the pace at which AIs are developed, we only have a very small window of time to act - and the open letter's aim is to slightly extend this window.

## What you, your organizations and our institutions can do
Installing a democratic control over today's most impactful algorithms is an urgent, enormous and [fabulous endeavor](https://pages.rts.ch/la-1ere/programmes/cqfd/11242341-comment-rendre-lintelligence-artificielle-benefique-27-04-2020.html). Achieving it in due time will require the help and participation of a large number of individuals, with all sorts of talents and expertise. This is why I would like to personally ask you to get involved.

A first challenge is attention. We must urgently pay a lot more attention to cybersecurity risks, and to make sure that our relatives, colleagues and institutions do the same. In particular, Big Tech employees must no longer be invited and celebrated, especially in universities and media, without being challenged about the security and the ethics of the products that fund them. Likewise, public universities must prioritize security and ethics research.

A second challenge is institutional. While new laws are needed, today's large-scale algorithms are likely already violating existing laws, like [profiting from ad-based scams](https://www.swissinfo.ch/eng/society/annual-stats_fraud-and-online-crime-on-the-rise-in-switzerland/45636712). However, the current complete lack of external oversight is preventing justice from being delivered. Regulatory agencies must urgently be set up and well-funded to enforce law online.

A third challenge lies in the development of democratically governed secure alternatives to today's most impactful algorithms. This is precisely what I have spent most of the last 5 five years working on, as my colleagues and I set up the nonprofit [Tournesol](https://tournesol.app/) project. Essentially, Tournesol's algorithm results from a secure and fair vote on its preferred behavior by Tournesol's community of contributors, which you are welcome to join.

Overall, the more we prioritize the safety of our information ecosystems, the more we will have a chance to make the Internet for all.	