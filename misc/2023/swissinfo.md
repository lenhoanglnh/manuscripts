# A democratic oversight on AI is urgently needed

#### We desperately need more attention, manpower and funding to set up governance systems akin to those introduced in the airline, pharmaceutical and food industries.

## AIs are out of (democratic) control
On March 29, an [open letter](https://futureoflife.org/open-letter/pause-giant-ai-experiments/) demanding to "pause giant AI experiments" was signed by over 2,500 academics and tech leaders. This call is long overdue. Two years earlier, my colleagues and I already called for a moratorium, based on mathematical security impossibility theorems, in [a paper](https://arxiv.org/abs/2209.15259) whose [publication was long delayed by Google](https://tournesol.app/entities/yt:bNfZ0yhVccw).

Over the last decade, impressive algorithms have been hastily developed and deployed on massive scales, like [ChatGPT](https://www.swissinfo.ch/eng/machines-and-ethics--artificial-intelligence-switzerland/46213634) and [MidJourney](https://www.theverge.com/2023/3/30/23662940/deepfake-viral-ai-misinformation-midjourney-stops-free-trials).
Similar AIs have been widely commercialized, for [fraud detection](https://www.swissinfo.ch/eng/business/fintech_ai-in-banking--the-reality-behind-the-hype/44046934), [resume filtering](https://www.forbes.com/sites/tomaspremuzic/2018/05/27/four-unethical-uses-of-ai-in-recruitment/), [video surveillance](https://www.swissinfo.ch/eng/reuters/china-uses-ai-software-to-improve-its-surveillance-capabilities/47501490) and [customer service](https://www.forbes.com/sites/forbesbusinessdevelopmentcouncil/2023/03/27/how-can-ai-fit-into-customer-service-call-centers-effectively/) (despite often known [shortcomings](https://www.wired.com/story/welfare-fraud-industry/) and [biases](https://www.theguardian.com/technology/2023/mar/27/robot-recruiters-can-bias-be-banished-from-ai-recruitment-hiring-artificial-intelligence)). But their main application is arguably in marketing. After all, many of today's biggest tech companies like Google, TikTok and Meta, mostly profit from ad targeting, while [ChatGPT's first public customer was none other than Coca-Cola](https://twitter.com/gdb/status/1628122763847413760).
This should raise public health and climate change [red flags](https://tournesol.app/entities/yt:sGLiSLAlwrY).

Additionally, algorithms have been shown to [spread misinformation](https://www.nytimes.com/2023/02/08/technology/ai-chatbots-disinformation.html), normalize [pseudo-medicines](https://www.fda.gov/consumers/consumer-updates/recipe-danger-social-media-challenges-involving-medicines), amplify [cyberbullying](https://c-hit.org/2019/08/12/social-medias-role-in-cyberbullying/), [endanger mental health](https://jonathanhaidt.substack.com/p/international-mental-illness-part-one), facilitate [illegal (even slavery) markets](https://edition.cnn.com/2021/10/25/tech/facebook-instagram-app-store-ban-human-trafficking/index.html), [fuel hate and anger](https://www.technologyreview.com/2021/10/05/1036519/facebook-whistleblower-frances-haugen-algorithms/), produce [deep fakes](https://slate.com/technology/2021/09/deepfake-video-scams.html), [be manipulated by state actors](https://forbiddenstories.org/story-killers/insider/), [destabilize democracies](https://www.theatlantic.com/ideas/archive/2022/07/social-media-harm-facebook-meta-response/670975/) and even contribute to genocides, as asserted by [the United Nations](https://www.theguardian.com/technology/2018/mar/13/myanmar-un-blames-facebook-for-spreading-hatred-of-rohingya) and [Amnesty International](https://www.amnesty.org/en/latest/news/2022/09/myanmar-facebooks-systems-promoted-violence-against-rohingya-meta-owes-reparations-new-report/).
Algorithms are threatening national security.

Yet their development is exceedingly opaque. Hardly any external entity can peek at Google, Meta or OpenAI's algorithms. Internal opposition forces have even been removed, as [Google fired its ethics team](https://www.swissinfo.ch/eng/business/what-happens-when-google-fires-its-ethics-/46472076), [Meta dismantled its responsible innovation team](https://www.wsj.com/articles/facebook-parent-meta-platforms-cuts-responsible-innovation-team-11662658423), and [Microsoft laid off an ethics team](https://techcrunch.com/2023/03/13/microsoft-lays-off-an-ethical-ai-team-as-it-doubles-down-on-openai/), after they alarmed about rushed deployment, as in the ["Stochastic Parrot" paper](https://www.theverge.com/2023/3/30/23662940/deepfake-viral-ai-misinformation-midjourney-stops-free-trials), in the [Facebook Files leak](https://www.wsj.com/articles/the-facebook-files-11631713039) or by [Twitter's former head of security](https://www.npr.org/2022/09/13/1122671582/twitter-whistleblower-mudge-senate-hearing).
Powerful profit-seeking companies have successfully engineered a world state where their algorithms can be designed with hardly any accountability.

## An effective AI governance is urgently needed
The software industry is far from being the first out-of-control technology industry. For decades, the airline, car, pharmaceutical, food, tobacco, construction and energy industries, among many others, have commercialized unchecked products, which have cost millions of lives. Public societies eventually opposed this alarming lack of accountability. In all democracies, strict laws and powerful well-funded regulatory agencies have been set up since to enforce a democratic control over these markets. A similar oversight on the software industry is long overdue.

In particular, we urgently need to favor *secure* and *ethical* technologies, rather than demand that our countries lead the race for eye-catching AIs. Concretely, the impressiveness of the algorithms 	running our smartgrids, cars, planes, power stations, banks, data centers, social networks and smartphones, should matter a lot less than their *cybersecurity*. As we warned in [a 2019 book](https://actu.epfl.ch/news/in-a-new-book-ic-scientists-draft-a-roadmap-for-be/), if these algorithms are [brittle](https://www.bbc.com/future/article/20170410-how-to-fool-artificial-intelligence), [vulnerable](https://www.theregister.com/2023/03/23/critical_infrastructure_hardware_flaws/), [backdoored](https://www.cnet.com/tech/mobile/us-finds-huawei-has-backdoor-access-to-mobile-networks-globally-report-says/) or [outsourced to an unreliable provider](https://www.cisecurity.org/solarwinds), or [if they abuse human rights](https://www.amnesty.org/en/latest/news/2023/03/france-intrusive-olympics-surveillance-technologies-could-usher-in-a-dystopian-future/), which [is usually the case](https://www.forbes.com/sites/bernardmarr/2023/02/06/cyber-apocalypse-2023-is-the-world-heading-for-a-catastrophic-event/), then we will all be in great danger. Nicole Perlroth's latest book sums it up clearly: [this is how they tell me the world ends](https://thisishowtheytellmetheworldends.com/).

Yet the software industry and academia, as well as the current legal and economic incentives, are mostly hampering the security mindset. Too often, the more cited, celebrated and funded researchers, the more paid software positions and the more successful companies, are those who do not invest much of their time and money on cybersecurity and ethics. As a growing number of experts reckon, this must change. Urgently.

Our democracies probably cannot afford the decades that were required to set up laws and inspection agencies in other industries. Given the pace at which new fancier algorithms are developed, we only have a very small window of time to act - the open letter aims to slightly extend this window.

## What you, your organizations and our institutions can do
Installing a democratic control over today's most critical algorithms is an urgent, enormous and [fabulous endeavor](https://pages.rts.ch/la-1ere/programmes/cqfd/11242341-comment-rendre-lintelligence-artificielle-benefique-27-04-2020.html), which will not be achieved in due time without the participation of a large number of individuals, with diverse talents, expertise and responsibilities. I would like to personally ask you to get involved.

A first challenge is attention. We must urgently pay a lot more attention to cybersecurity risks, and to make sure that our relatives, colleagues and institutions do the same. Any discussion on an information technology should be accompanied with the question "what can go wrong?", while Big Tech employees must no longer be invited and celebrated, especially in universities and media, without being challenged about the security and the ethics of the products that fund them. 

A second challenge is institutional. While new laws are needed, today's large-scale algorithms are likely already violating existing laws, like [profiting from ad-based scams](https://www.ftc.gov/news-events/news/press-releases/2023/03/ftc-issues-orders-social-media-video-streaming-platforms-regarding-efforts-address-surge-advertising). However, [the current complete lack of external oversight is preventing justice from being delivered](https://tournesol.app/entities/yt:Sqa8Zo2XWc4). Regulatory agencies must urgently be set up and well-funded to enforce law online.

A third challenge lies in the development of democratically-governed secure alternatives to today's most impactful algorithms. This is what I have spent most of the last five years working on, as my colleagues and I set up the nonprofit [Tournesol](https://tournesol.app/) project. Essentially, Tournesol's algorithm results from a secure and fair vote on its preferred behavior by Tournesol's community of contributors, which you are welcome to join.

Overall, the quicker you prioritize the security of our information ecosystems, the more we will have a chance to protect our societies against our current massive cybersecurity vulnerabilities.				